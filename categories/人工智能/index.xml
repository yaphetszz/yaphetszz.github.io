<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>人工智能 on Y4ph3tS blog</title>
    <link>https://yaphetszz.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/</link>
    <description>Recent content in 人工智能 on Y4ph3tS blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Tue, 04 Feb 2025 22:30:00 +0000</lastBuildDate>
    <atom:link href="https://yaphetszz.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deepseek-R1本地部署过程</title>
      <link>https://yaphetszz.github.io/posts/%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2deepseek/</link>
      <pubDate>Tue, 04 Feb 2025 22:30:00 +0000</pubDate>
      <guid>https://yaphetszz.github.io/posts/%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2deepseek/</guid>
      <description>&lt;p&gt;最近很多人都在研究本地大模型的部署，我也来踩个坑，其实部署方式很简单，本篇主要记录了本地部署和本地图形化UI工具的部署过程&lt;/p&gt;&#xA;&lt;p&gt;访问https://ollama.com/download下载ollama&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../ollma/image-20250204213106554.png&#34; alt=&#34;image-20250204213106554&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;下载完成后进行默认安装，之后任务栏里会出现托盘图标&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../ollma/image-20250204213654426.png&#34; alt=&#34;image-20250204213654426&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;起一个新的CMD窗口看看是否已经自动配好了环境变量&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../ollma/image-20250204213747881.png&#34; alt=&#34;image-20250204213747881&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;接下来确定一下本地的GPU版本，可以用一些工具如GPU-z等，我用了MATLAB自带的gpuinfo进行查看，我的GPU型号为NVIDIA RTX 3080Ti，有点老了，毕竟电脑用了好多年了已经&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../ollma/image-20250204214042149.png&#34; alt=&#34;image-20250204214042149&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;以下为网上找的一个适配版本的图，可以参考一下自己的GPU能够跑哪些版本的模型&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../ollma/image-20250204214334125.png&#34; alt=&#34;image-20250204214334125&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;如果已经可以正常使用，就可以通过以下命令进行安装，稳妥起见用了1.5b，如果用7B其实也问题不大，&lt;del&gt;经测试如果让显卡能烤地瓜的话8B也能跑，别说是我说的&lt;/del&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ollama run deepseek-r1:1.5b&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;安装完成后如下图，可以直接提问，且响应速度还挺快的&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../ollma/image-20250204220121827.png&#34; alt=&#34;image-20250204220207394&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;接下来配置一下本地UI，使用chatbox这个工具进行，地址为https://chatboxai.app/zh&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../ollma/image-20250204215303052.png&#34; alt=&#34;image-20250204215303052&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;安装完成后进行配置，首先创建OLLAMA的环境变量，确保能够访问到服务&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../ollma/image-20250204220417911.png&#34; alt=&#34;image-20250204220417911&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;然后再chatbox中进行配置&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../ollma/image-20250204215412626.png&#34; alt=&#34;image-20250204215412626&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;选择ollama的API&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../ollma/image-20250204215453672.png&#34; alt=&#34;image-20250204215453672&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;选择本地模型&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../ollma/image-20250204220505613.png&#34; alt=&#34;image-20250204220505613&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;此时已经成功连接了本地的ollama&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../ollma/image-20250204220548559.png&#34; alt=&#34;image-20250204220548559&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;这时候一些简单的东西在无互联网连接的情况下也能正常回答，非常流畅可以说是&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../ollma/image-20250204221112791.png&#34; alt=&#34;image-20250204221112791&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>神经网络基础</title>
      <link>https://yaphetszz.github.io/posts/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Thu, 30 Jan 2025 16:08:36 +0800</pubDate>
      <guid>https://yaphetszz.github.io/posts/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</guid>
      <description>&lt;p&gt;最近AI挺火的，重新开始学习一下神经网络并且整理一些东西，简单写点笔记记录一下。&lt;/p&gt;&#xA;&lt;p&gt;神经网络是一种模仿生物神经系统结构和功能的计算模型，主要用于机器学习和人工智能领域。它由大量相互连接的节点（称为“神经元”）组成，这些节点通过加权连接传递信息。神经网络的核心思想是通过调整这些连接的权重来学习数据中的模式。&lt;/p&gt;&#xA;&lt;h3 id=&#34;主要组成部分&#34;&gt;主要组成部分&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;输入层&lt;/strong&gt;：接收外部输入数据。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;：位于输入层和输出层之间，负责处理数据。可以有多个隐藏层。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;输出层&lt;/strong&gt;：生成最终的输出结果。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;工作原理&#34;&gt;工作原理&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;前向传播&lt;/strong&gt;：输入数据通过各层传递，每层对数据进行加权和激活函数处理，最终生成输出。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;激活函数&lt;/strong&gt;：引入非线性，使网络能够学习复杂模式，常用函数包括ReLU、Sigmoid和Tanh。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;损失函数&lt;/strong&gt;：衡量预测结果与实际结果的差距。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;反向传播&lt;/strong&gt;：通过梯度下降法调整权重，最小化损失函数。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;优化算法&lt;/strong&gt;：如SGD、Adam，用于更新权重。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;主要类型&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;前馈神经网络（FNN）&lt;/strong&gt;：信息单向传递，无循环。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;卷积神经网络（CNN）&lt;/strong&gt;：适用于图像处理，通过卷积层提取特征。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;循环神经网络（RNN）&lt;/strong&gt;：处理序列数据，具有记忆功能。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;长短期记忆网络（LSTM）&lt;/strong&gt;：RNN的改进，解决长序列依赖问题。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;生成对抗网络（GAN）&lt;/strong&gt;：由生成器和判别器组成，用于生成数据。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;网上搜了些文章资料，大致总结出的学习内容，数学基础：了解线性代数、微积分和概率论，特别是矩阵运算、导数和梯度等概念。编程基础：熟悉Python，掌握NumPy、Pandas等数据处理库。&lt;/p&gt;&#xA;&lt;h3 id=&#34;神经网络中的数学基础&#34;&gt;神经网络中的数学基础&lt;/h3&gt;&#xA;&lt;h3 id=&#34;1-线性代数&#34;&gt;1. 线性代数&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;向量&lt;/strong&gt;：一维数组，可以表示输入数据或权重。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;矩阵&lt;/strong&gt;：二维数组，常用于表示权重矩阵或输入数据。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：&#xA;假设有一个简单的神经网络，输入层有2个神经元，隐藏层有3个神经元。输入数据可以表示为一个向量：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../NeuralNetwork/image-20250130231136971.png&#34; alt=&#34;image-20250130231136971&#34;&gt;&#xA;隐藏层的权重可以表示为一个矩阵：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../NeuralNetwork/image-20250130231301044.png&#34; alt=&#34;image-20250130231301044&#34;&gt;&lt;/p&gt;&#xA;&lt;h4 id=&#34;矩阵乘法&#34;&gt;矩阵乘法&lt;/h4&gt;&#xA;&lt;p&gt;矩阵乘法用于前向传播中计算神经元的加权输入。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：&#xA;计算隐藏层的输入：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../NeuralNetwork/image-20250130231310393.png&#34; alt=&#34;image-20250130231310393&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-微积分&#34;&gt;2. &lt;strong&gt;微积分&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;导数&#34;&gt;导数&lt;/h4&gt;&#xA;&lt;p&gt;导数用于计算损失函数对权重的梯度。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：&#xA;假设损失函数为均方误差（MSE）：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../NeuralNetwork/image-20250130231330044.png&#34; alt=&#34;image-20250130231330044&#34;&gt;&#xA;其中，y^是预测值，y是真实值。损失函数对预测值的导数为：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../NeuralNetwork/image-20250130231355084.png&#34; alt=&#34;image-20250130231355084&#34;&gt;&lt;/p&gt;&#xA;&lt;h4 id=&#34;链式法则&#34;&gt;链式法则&lt;/h4&gt;&#xA;&lt;p&gt;链式法则用于反向传播中计算梯度。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：&#xA;假设有一个简单的神经网络，隐藏层的输出为 z，激活函数为 a=σ(z)，损失函数为 L。根据链式法则，损失函数对权重的导数为：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../NeuralNetwork/image-20250130233912421.png&#34; alt=&#34;image-20250130233912421&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;3-概率与统计&#34;&gt;3. &lt;strong&gt;概率与统计&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;概率分布&#34;&gt;概率分布&lt;/h4&gt;&#xA;&lt;p&gt;理解数据分布有助于设计损失函数和评估模型。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：&#xA;假设我们有一个二分类问题，输出层的激活函数为Sigmoid函数，输出值可以解释为概率：&lt;/p&gt;&#xA;&lt;p&gt;P(y=1∣x)=σ(z)P(y=1∣x)=σ(z)&lt;/p&gt;&#xA;&lt;h4 id=&#34;最大似然估计&#34;&gt;最大似然估计&lt;/h4&gt;&#xA;&lt;p&gt;最大似然估计用于定义损失函数。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：&#xA;对于二分类问题，常用的损失函数为交叉熵损失：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../NeuralNetwork/image-20250130233929606.png&#34; alt=&#34;image-20250130233929606&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;4-优化&#34;&gt;4. &lt;strong&gt;优化&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;梯度下降&#34;&gt;梯度下降&lt;/h4&gt;&#xA;&lt;p&gt;梯度下降通过计算损失函数的梯度并更新权重来最小化损失。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：&#xA;假设损失函数对权重的梯度为&lt;img src=&#34;../NeuralNetwork/image-20250130233953350.png&#34; alt=&#34;image-20250130233953350&#34;&gt;，学习率为 η，则权重的更新公式为：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
