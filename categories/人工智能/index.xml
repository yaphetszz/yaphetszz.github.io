<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>人工智能 on Y4ph3tS blog</title>
    <link>https://yaphetszz.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/</link>
    <description>Recent content in 人工智能 on Y4ph3tS blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 30 Jan 2025 16:08:36 +0800</lastBuildDate>
    <atom:link href="https://yaphetszz.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>神经网络基础</title>
      <link>https://yaphetszz.github.io/posts/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Thu, 30 Jan 2025 16:08:36 +0800</pubDate>
      <guid>https://yaphetszz.github.io/posts/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</guid>
      <description>&lt;p&gt;最近AI挺火的，重新开始学习一下神经网络并且整理一些东西，简单写点笔记记录一下。&lt;/p&gt;&#xA;&lt;p&gt;神经网络是一种模仿生物神经系统结构和功能的计算模型，主要用于机器学习和人工智能领域。它由大量相互连接的节点（称为“神经元”）组成，这些节点通过加权连接传递信息。神经网络的核心思想是通过调整这些连接的权重来学习数据中的模式。&lt;/p&gt;&#xA;&lt;h3 id=&#34;主要组成部分&#34;&gt;主要组成部分&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;输入层&lt;/strong&gt;：接收外部输入数据。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;隐藏层&lt;/strong&gt;：位于输入层和输出层之间，负责处理数据。可以有多个隐藏层。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;输出层&lt;/strong&gt;：生成最终的输出结果。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;工作原理&#34;&gt;工作原理&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;前向传播&lt;/strong&gt;：输入数据通过各层传递，每层对数据进行加权和激活函数处理，最终生成输出。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;激活函数&lt;/strong&gt;：引入非线性，使网络能够学习复杂模式，常用函数包括ReLU、Sigmoid和Tanh。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;损失函数&lt;/strong&gt;：衡量预测结果与实际结果的差距。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;反向传播&lt;/strong&gt;：通过梯度下降法调整权重，最小化损失函数。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;优化算法&lt;/strong&gt;：如SGD、Adam，用于更新权重。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;主要类型&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;前馈神经网络（FNN）&lt;/strong&gt;：信息单向传递，无循环。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;卷积神经网络（CNN）&lt;/strong&gt;：适用于图像处理，通过卷积层提取特征。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;循环神经网络（RNN）&lt;/strong&gt;：处理序列数据，具有记忆功能。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;长短期记忆网络（LSTM）&lt;/strong&gt;：RNN的改进，解决长序列依赖问题。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;生成对抗网络（GAN）&lt;/strong&gt;：由生成器和判别器组成，用于生成数据。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;网上搜了些文章资料，大致总结出的学习内容，数学基础：了解线性代数、微积分和概率论，特别是矩阵运算、导数和梯度等概念。编程基础：熟悉Python，掌握NumPy、Pandas等数据处理库。&lt;/p&gt;&#xA;&lt;h3 id=&#34;神经网络中的数学基础&#34;&gt;神经网络中的数学基础&lt;/h3&gt;&#xA;&lt;h3 id=&#34;1-线性代数&#34;&gt;1. 线性代数&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;向量&lt;/strong&gt;：一维数组，可以表示输入数据或权重。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;矩阵&lt;/strong&gt;：二维数组，常用于表示权重矩阵或输入数据。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：&#xA;假设有一个简单的神经网络，输入层有2个神经元，隐藏层有3个神经元。输入数据可以表示为一个向量：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../NeuralNetwork/image-20250130231136971.png&#34; alt=&#34;image-20250130231136971&#34;&gt;&#xA;隐藏层的权重可以表示为一个矩阵：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../NeuralNetwork/image-20250130231301044.png&#34; alt=&#34;image-20250130231301044&#34;&gt;&lt;/p&gt;&#xA;&lt;h4 id=&#34;矩阵乘法&#34;&gt;矩阵乘法&lt;/h4&gt;&#xA;&lt;p&gt;矩阵乘法用于前向传播中计算神经元的加权输入。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：&#xA;计算隐藏层的输入：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../NeuralNetwork/image-20250130231310393.png&#34; alt=&#34;image-20250130231310393&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;2-微积分&#34;&gt;2. &lt;strong&gt;微积分&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;导数&#34;&gt;导数&lt;/h4&gt;&#xA;&lt;p&gt;导数用于计算损失函数对权重的梯度。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：&#xA;假设损失函数为均方误差（MSE）：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../NeuralNetwork/image-20250130231330044.png&#34; alt=&#34;image-20250130231330044&#34;&gt;&#xA;其中，y^是预测值，y是真实值。损失函数对预测值的导数为：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../NeuralNetwork/image-20250130231355084.png&#34; alt=&#34;image-20250130231355084&#34;&gt;&lt;/p&gt;&#xA;&lt;h4 id=&#34;链式法则&#34;&gt;链式法则&lt;/h4&gt;&#xA;&lt;p&gt;链式法则用于反向传播中计算梯度。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：&#xA;假设有一个简单的神经网络，隐藏层的输出为 z，激活函数为 a=σ(z)，损失函数为 L。根据链式法则，损失函数对权重的导数为：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../NeuralNetwork/image-20250130233912421.png&#34; alt=&#34;image-20250130233912421&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;3-概率与统计&#34;&gt;3. &lt;strong&gt;概率与统计&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;概率分布&#34;&gt;概率分布&lt;/h4&gt;&#xA;&lt;p&gt;理解数据分布有助于设计损失函数和评估模型。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：&#xA;假设我们有一个二分类问题，输出层的激活函数为Sigmoid函数，输出值可以解释为概率：&lt;/p&gt;&#xA;&lt;p&gt;P(y=1∣x)=σ(z)P(y=1∣x)=σ(z)&lt;/p&gt;&#xA;&lt;h4 id=&#34;最大似然估计&#34;&gt;最大似然估计&lt;/h4&gt;&#xA;&lt;p&gt;最大似然估计用于定义损失函数。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：&#xA;对于二分类问题，常用的损失函数为交叉熵损失：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../NeuralNetwork/image-20250130233929606.png&#34; alt=&#34;image-20250130233929606&#34;&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;4-优化&#34;&gt;4. &lt;strong&gt;优化&lt;/strong&gt;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;梯度下降&#34;&gt;梯度下降&lt;/h4&gt;&#xA;&lt;p&gt;梯度下降通过计算损失函数的梯度并更新权重来最小化损失。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;例子&lt;/strong&gt;：&#xA;假设损失函数对权重的梯度为&lt;img src=&#34;../NeuralNetwork/image-20250130233953350.png&#34; alt=&#34;image-20250130233953350&#34;&gt;，学习率为 η，则权重的更新公式为：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
